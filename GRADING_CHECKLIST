# Grading Checklist - Week 6 & 7 Combined Homework

**Team Name:** AI4ce  
**Project Title:** CodeMentor AI - Technical Interview Prep with Personalized Weakness Detection  
**Submission Date:** November 26, 2025  
**Team Members:** [Add your team member names here]

---

## Part A: Week 6 - Function Calling (110 points)

### Functions Implementation (40 points)
- [x] **Function 1:** analyze_code_submission - Located at: `src/backend/functions/tools.py`
- [x] **Function 2:** get_recommended_problem - Located at: `src/backend/functions/tools.py`
- [x] **Function 3:** track_user_progress - Located at: `src/backend/functions/tools.py`
- [x] Pydantic models defined at: `src/backend/models/function_models.py`
- [x] JSON schemas defined at: `src/ai/agent.py`

**Quick Navigation:**
- Functions code: [https://github.com/YOUR_USERNAME/YOUR_REPO/blob/main/src/backend/functions/tools.py]
- Models code: [https://github.com/YOUR_USERNAME/YOUR_REPO/blob/main/src/backend/models/function_models.py]
- Agent/orchestration: [https://github.com/YOUR_USERNAME/YOUR_REPO/blob/main/src/ai/agent.py]

### Tests & Demo (30 points)
- [x] Test file located at: `tests/test_functions.py`
- [x] All tests pass (screenshot/evidence at: `docs/evidence/test-results.png`)
- [ ] Demo video link: [ADD YOUR YOUTUBE/DRIVE LINK HERE]
- [ ] Demo video shows: ✅ AI calling functions ✅ Multiple scenarios ✅ Error handling

### Documentation & Code Quality (20 points)
- [x] README updated with Week 6 section: [Link to your README#week-6]
- [x] All functions have docstrings: ✅ Yes
- [x] Type hints present: ✅ Yes
- [x] Error handling implemented: ✅ Yes
- [x] No hard-coded API keys: ✅ Confirmed

### GitHub (10 points)
- [x] Descriptive commit messages used: ✅ Yes
- [x] .env is NOT in repo: ✅ Confirmed
- [x] .env.example exists: ✅ Yes
- [x] All files pushed to main branch: ✅ Confirmed

### Evaluation & Safety Logs (10 points)
Located in: `course-pack/labs/lab-6/`
- [x] `evaluation_notes.md`: [Link to file]
- [x] `safety_checklist.md`: [Link to file]
- [x] `ai_use_log.md`: [Link to file]
- [x] `capstone_link.md`: [Link to file]

---

## Part B: Week 7 - Design Review (5 points)

### Main Documents
- [x] Design Review Document: `docs/design-review-week7.md` - [Link to file]
- [x] Event Schemas: `docs/event-schemas.md` - [Link to file]
- [x] Updated Architecture Diagram: `docs/architecture-week7.png` - [Link to file]

### Design Review Sections Completed (check all 6)
- [x] Section 1: Architecture Validation (1-2 pages)
- [x] Section 2: Event Schema Documentation (2-3 pages)
- [x] Section 3: Smoke Test Results (1-2 pages)
- [x] Section 4: Performance Baseline (1 page)
- [x] Section 5: Hypothesis Validation (1-2 pages)
- [x] Section 6: Readiness Assessment (1 page)

### Evidence Folder
- [x] Evidence folder exists at: `docs/evidence/`
- [x] Contains: Logs, screenshots, performance data, validation results

### README Update
- [x] README links to design review: [Link to README section]
- [x] README notes architectural changes (if any): ✅ No major changes from Week 2

---

## Additional Notes for Grader

**Strengths of our implementation:**
- All 17 tests passing with 0 failures
- Average latencies well under target (8-46ms)
- Comprehensive error handling with fallbacks
- Clear separation of concerns (models, functions, agent)
- Documented AI usage transparently

**Known limitations (being addressed):**
- Currently using mock data for test execution (Judge0 integration planned for Week 7)
- No database persistence yet (PostgreSQL integration planned for Week 8)
- Frontend not yet implemented (React + Monaco planned for Week 9)

**Next steps:**
1. Integrate Judge0 API for real code execution
2. Add PostgreSQL database for user profiles
3. Build React frontend with code editor
4. Deploy to production environment
