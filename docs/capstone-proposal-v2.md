# Refined Capstone Proposal (Version 2)

**Project Name:** CodeMentor AI  
**Team Name:** AI4ce  
**Team Members:** [Your team members]  
**Date:** Week 4, October 25, 2025  
**Version:** 2.0 (Updated from Week 2 submission)

---

## Document Change Log

### What Changed Since Week 2?

| Section | Change Type | Summary |
|---------|-------------|---------|
| Problem Statement | **Refined** | The focus was expanded from CS students to include all candidates preparing for interviews. |
| Technical Architecture | **Major Update** | Switched from custom ML to hybrid approach (GPT-4 + simple classifier) |
| Success Criteria | **Enhanced** | Added measurable metrics with specific targets |
| Risk Assessment | **Expanded** | Added 3 new risks discovered during prototyping |
| Timeline | **Realistic** | Adjusted based on Week 3-4 velocity and code execution complexity |

---

## 1. Problem Statement (Updated)

### The Problem

**Original (Week 2):**
> Many cs students fail technical interviews because they have blind spots they don't recognize.

**Refined (Week 4):**

candiates preparing for technical interviews face a critical problem: **they practice blindly without understanding their systematic weaknesses**. Through examining online communities, we confirmed that:

- candiates solve 50-100 LeetCode problems but keep making the same mistakes (e.g., always choosing O(n¬≤) solutions, missing edge cases for empty arrays)
- LeetCode only shows "Accepted" or "Wrong Answer" without analyzing *why* their approach is suboptimal
- they waste time on random problems instead of targeting their actual gaps
- With only 2-3 interview attempts before 6-12 month waiting periods, inefficient practice is costly

**Why AI is the Right Solution:**

Traditional platforms are passive problem banks. AI can:
- **Analyze code patterns** beyond correctness (algorithmic thinking, edge case handling, code structure)
- **Detect recurring mistakes** across submissions to identify systematic weaknesses (e.g., "You miss null checks 80% of the time")
- **Generate personalized explanations** connecting mistakes to CS concepts
- **Adapt difficulty** dynamically based on performance

**Scope Validation (Week 3-4):**
- Code execution sandbox works (tested Judge0 API)
- Basic AST parsing functional (Python only for MVP)
- Custom ML model too complex for timeline ‚Üí Switching to simpler rule-based + GPT-4 hybrid

---

## 2. Target Users (Updated)

### Primary User Persona

**User: "Alex" the CS Junior**

- **Demographics:** 20-22 years old, CS major (junior/senior), moderate coding experience (completed Data Structures & Algorithms)
- **Role/Context:** Preparing for summer internships at FAANG companies, practicing 1-2 hours daily for 8-12 weeks before interview season
- **Goals:** 
  - Identify blind spots before real interviews
  - Practice efficiently (work on weaknesses, not random problems)
  - Understand *why* solutions are suboptimal
  - Track measurable improvement
- **Pain Points:** 
  - "I solve problems but don't know if I'm actually improving"
  - "LeetCode doesn't tell me why my approach is wrong, just that it is"
  - "I run out of time even when I know the solution"
  - "I keep making the same mistakes but don't realize it"
- **Current Behavior:** Uses LeetCode, watches NeetCode videos, practices random Easy/Medium problems
- **Success Criteria:** Would use regularly if it clearly identifies 3+ recurring weaknesses and shows improvement over 2 weeks

### User Validation (Week 3-4)

**How We Validated:**
- Observed several students solving problems on LeetCode
- Posted in Discord groups, got 12 responses confirming problem

**Key Insights:**
1. **everybody** confirmed they don't know their specific weaknesses: "I just know I'm bad at interviews"
2. **everybody** said personalized problem recommendations would be "extremely valuable"
3. **they want concrete progress metrics**, not vague feedback like "good job"
4. **Time pressure is critical** - most of them mentioned running out of time more than getting wrong answers

---

## 3. Success Criteria (Updated & Measurable)

### Product Success Metrics

| Metric | Target (Week 15) | Measurement Method | Baseline (Week 4) | Purpose / Justification |
|--------|-----------------|-----------------|-----------------|------------------------|
| Weakness Detection Accuracy | ‚â•75% | User survey (‚ÄúWas this weakness accurate?‚Äù after 5 problems) | Not measured | Build trust in feedback system |
| Problem Recommendation Relevance | ‚â•70% | Post-solve rating (‚ÄúWas this problem helpful for your gaps?‚Äù) | Not measured | Ensures personalized learning value |
| Task Completion Rate | ‚â•80% | % of users completing full solve ‚Üí feedback ‚Üí next problem loop | Not measured | Validates usability & engagement |
| Performance Improvement | ‚â•25% reduction in repeated error patterns | Compare first 5 vs. next 10 problems | Not measured | Demonstrates measurable learning impact |
| User Satisfaction | ‚â•4.0/5.0 | Post-testing survey (Weeks 7 & 14) | Not measured | Gauges overall product experience |
| Interview Confidence (Proxy) | ‚â•70% report feeling more confident | End-of-cycle survey | Not measured | Indicates real-world readiness |

### Technical Success Metrics

| Metric | Target | Measurement Method | Current Performance | Purpose |
|--------|--------|------------------|------------------|---------|
| Code Execution Success Rate | ‚â•95% | % of submissions executed successfully | ~85% (Judge0 timeouts) | Ensure reliability of sandbox |
| Feedback Generation Time | <10s | Time from submission ‚Üí displayed feedback | ~12s | Maintain user engagement |
| Pattern Detection Coverage | ‚â•5 distinct error types per user | Analyzed via AST + ML classifier | Not yet measured | Validates insight depth |
| API Cost per Query | ‚â§$0.10 | Track GPT API costs | ~0.015/test | Guarantee scalability |
| AST Parsing Accuracy | ‚â•90% | % of valid Python code successfully parsed | ~95% | Confirms technical robustness |

### Learning Goals (Team-Level)

| Team Member | Learning Objective | Success Criteria | Progress (Week 4) |
|------------|------------------|----------------|-----------------|
| Member 1 | Master code execution sandboxing and resource isolation | Judge0 integrated with custom timeouts & limits | ‚úÖ Basic setup complete |
| Member 2 | Develop AST-based feature extraction pipeline | Extract ‚â•10 relevant code features | ‚úÖ Parser functional |
| Member 3 | Build adaptive recommendation engine | ‚â•70% of users rate suggestions as ‚Äúhelpful‚Äù | ‚öôÔ∏è Scheduled for Week 8 |
| Member 4 | Design data visualization dashboard | Display progress metrics (error reduction, accuracy trend) | üîÑ Prototype in progress |

**Why These Metrics Matter**
- 75% Weakness Accuracy: Ensures user trust ‚Äî inaccurate feedback destroys credibility  
- <10s Feedback Time: Prevents drop-offs during practice  
- 25% Improvement: Confirms measurable skill growth  
- 80% Completion Rate: Indicates intuitive and engaging core flow  
- 70% Confidence Gain: Reflects real-world interview readiness ‚Äî ultimate success indicator
  
---

## 4. Technical Architecture (Updated)

### Architecture Evolution

**What Changed Since Week 2:**

| Component | Week 2 Plan | Week 4 Reality | Why We Changed |
|-----------|-------------|----------------|----------------|
| **ML Model** | Custom Random Forest for error classification | Rule-based classifier + GPT-4 for explanations | Custom ML requires 500+ labeled examples; don't have time to collect/label |
| **Code Execution** | Uncertain between Judge0/Piston | Judge0 API | Tested both, Judge0 more reliable and better documented |
| **Scope** | 13 error categories | 6 core error categories | Realistic for timeline; can expand post-course |
| **Language Support** | Python + JavaScript | Python only (MVP) | JavaScript AST parsing more complex; defer to future work |
| **Database** | PostgreSQL for everything | PostgreSQL + Redis cache | Redis needed for problem recommendation performance |

### Current Architecture Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   User      ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Frontend      ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ    Backend      ‚îÇ
‚îÇ  (Browser)  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  (React +       ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  (FastAPI)      ‚îÇ
‚îÇ             ‚îÇ      ‚îÇ   Monaco Editor) ‚îÇ      ‚îÇ                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                        ‚îÇ
                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                          ‚ñº                             ‚ñº                    ‚ñº
                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                  ‚îÇ Judge0 API    ‚îÇ          ‚îÇ  Analysis Engine ‚îÇ  ‚îÇ PostgreSQL  ‚îÇ
                  ‚îÇ (Code         ‚îÇ          ‚îÇ  ‚Ä¢ AST Parser    ‚îÇ  ‚îÇ (User       ‚îÇ
                  ‚îÇ  Execution)   ‚îÇ          ‚îÇ  ‚Ä¢ Rule-Based    ‚îÇ  ‚îÇ  profiles,  ‚îÇ
                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ    Classifier    ‚îÇ  ‚îÇ  attempts)  ‚îÇ
                                             ‚îÇ  ‚Ä¢ Complexity    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                             ‚îÇ    Analyzer      ‚îÇ
                                             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                        ‚îÇ
                                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                          ‚ñº                            ‚ñº
                                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                ‚îÇ  OpenAI API      ‚îÇ        ‚îÇ  Redis Cache     ‚îÇ
                                ‚îÇ  (GPT-4 for      ‚îÇ        ‚îÇ  (Problem queue, ‚îÇ
                                ‚îÇ   explanations)  ‚îÇ        ‚îÇ   recommendations)‚îÇ
                                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Data Flow for Core User Action

**User Flow: Submit Solution**

```
1. User writes code in Monaco Editor
   ‚Üì
2. User clicks "Submit"
   ‚Üì
3. Frontend sends code + problem_id to backend
   ‚Üì
4. Backend (FastAPI):
   a. Validates input (syntax check, timeout limits)
   b. Sends code to Judge0 API
   c. Gets execution results (pass/fail for each test case)
   d. If failed: AST parser analyzes code structure
   e. Rule-based classifier detects error patterns:
      - Check for missing edge case handling (if/else for null, empty)
      - Analyze time complexity (nested loops = O(n¬≤))
      - Check for optimal data structure usage
   f. Store patterns in user weakness profile (PostgreSQL)
   g. Send patterns to GPT-4 for explanation generation
   h. Return to frontend: {result, patterns, explanation, next_problem}
   ‚Üì
5. Frontend displays:
   - Test results (‚úì or ‚úó)
   - Detected weaknesses with explanations
   - Personalized next problem recommendation
```

**Latency Budget:**

| Step | Target | Current | Status |
|------|--------|---------|--------|
| Code submission | <500ms | ~300ms | On track |
| Judge0 execution | <5s | ~6s | Sometimes slow |
| AST parsing | <500ms | ~200ms | On track |
| Pattern detection | <1s | ~800ms | On track |
| GPT-4 explanation | <3s | ~4s | Need prompt optimization |
| Database write | <200ms | ~150ms | On track |
| **Total (P95)** | **<10s** | **~12s** | Must optimize |

### Technology Stack

**Frontend:**
- React with TypeScript
- Monaco Editor (VS Code's editor component)
- Recharts for progress dashboard
- Hosting: Vercel

**Backend:**
- FastAPI (Python 3.11)
- Judge0 API for code execution
- Python `ast` module for AST parsing
- Hosting: Railway

**AI/ML:**
- GPT-4 for explanation generation (~$0.015/query)
- Rule-based classifier for pattern detection (no ML training needed initially)

**Data Storage:**
- PostgreSQL (user profiles, submission history)
- Redis (problem recommendation queue, caching)

**Why This Simplified Approach:**

| Original Plan | Simplified Approach | Rationale |
|---------------|-------------------|-----------|
| Train custom ML model on 500 examples | Rule-based classifier with 6 heuristics | Don't have time to collect/label 500 examples; rules cover 80% of cases |
| 13 error categories | 6 core categories | Realistic for MVP; can add more post-course |
| Python + JavaScript | Python only | One AST parser is enough complexity for 13 weeks |

### 6 Core Error Patterns (Simplified)

1. **Missing edge cases** (null, empty array, single element)
2. **Suboptimal time complexity** (O(n¬≤) when O(n) possible with hash map)
3. **Wrong data structure** (array when hash map/set is better)
4. **Off-by-one errors** (range/loop boundaries)
5. **Missing input validation** (no checks for constraints)
6. **Inefficient nested loops** (can be optimized)

**Detection Logic (Rule-Based):**
```python
# Pseudocode
if no null checks and problem has nullable inputs:
    patterns.append("missing_edge_case_null")

if nested loops and problem solvable in O(n) with hash map:
    patterns.append("suboptimal_complexity")

if using list for lookup and >10 lookups:
    patterns.append("wrong_data_structure")
```

### Security & Cost Controls

**Implemented (Week 4):**
- Judge0 execution timeouts (5 seconds max)
- Memory limits (128MB per execution)
- Rate limiting (20 submissions/hour per user)
- Input validation (max code length 5000 characters)
- Cost tracking dashboard (monitors GPT-4 usage)

**Planned (Week 5-6):**
- Prompt optimization (reduce tokens from 500 to 250)
- Caching repeated explanations for same error patterns

### Known Technical Debt

1. **No automated tests** (planned Week 6)
2. **Judge0 occasionally slow** (need fallback or optimization)
3. **No monitoring/logging** (planned Week 8: Sentry)
4. **Prompts hardcoded** (should move to config file)
5. **No database backups** (planned Week 7)

---

## 5. Risk Assessment (Updated & Expanded)

### Risk Matrix

| Risk ID | Risk | Likelihood | Impact | Severity | Status |
|---------|------|------------|--------|----------|--------|
| R1 | Judge0 API unreliability | HIGH | HIGH | üî¥ CRITICAL | Monitoring |
| R2 | Pattern detection accuracy <70% | MEDIUM | HIGH | üî¥ CRITICAL | Testing |
| R3 | GPT-4 costs exceed budget | MEDIUM | MEDIUM | üü° HIGH | Mitigating |
| R4 | AST parsing fails on edge cases | MEDIUM | MEDIUM | üü° HIGH | Testing |
| R5 | Users don't find recommendations helpful | HIGH | MEDIUM | üü° HIGH | Will validate Week 7 |
| R6 | Scope creep (too many features) | HIGH | MEDIUM | üü° HIGH | Controlled |
| R7 | Team member unavailability | MEDIUM | MEDIUM | üü¢ MEDIUM | Planning |

### Critical Risks (Detailed)

#### üî¥ Risk R1: Judge0 API Unreliability

**Description:** Judge0 occasionally times out or returns errors, blocking core functionality.

**Likelihood:** HIGH (experienced 15% failure rate in testing)  
**Impact:** HIGH (no execution = app is useless)  
**Severity:** CRITICAL

**What We've Learned (Week 3-4):**
- Judge0 free tier sometimes slow (6-8 seconds)
- Occasional 503 errors during peak hours
- Some valid Python code rejected due to Judge0 restrictions

**Preventive Mitigation:**
1. **DONE:** Added retry logic (3 attempts with exponential backoff)
2. **DONE:** Implemented timeout handling with user-friendly errors
3. **PLANNED (Week 5):** Upgrade to Judge0 paid tier ($10/month, more reliable)
4. **PLANNED (Week 6):** Add fallback to Piston API if Judge0 fails

**Contingency Plan:**
- If Judge0 fails consistently: Switch entirely to Piston API (tested as backup)
- If both fail: Reduce scope to "analysis only" mode (no execution, just analyze code structure)

**Monitoring:**
- Track execution success rate daily
- Alert if success rate drops below 90%

**Owner:** [Backend Lead]  
**Next Review:** Week 5

---

#### üî¥ Risk R2: Pattern Detection Accuracy <70%

**Description:** If rule-based classifier incorrectly identifies weaknesses, users won't trust system.

**Likelihood:** MEDIUM (rules are simple but might miss nuances)  
**Impact:** HIGH (destroys user trust and product value)  
**Severity:** CRITICAL

**What We've Learned (Week 4 Testing):**
- Tested on 15 sample solutions
- Accuracy: ~65% (10/15 correct classifications)
- Common failures: Misidentifying optimal solutions as suboptimal

**Preventive Mitigation:**
1.  **PLANNED (Week 5):** Create golden set with 30 problems + solutions
2.  **PLANNED (Week 6):** Refine rules based on golden set testing
3.  **PLANNED (Week 7):** User testing - validate accuracy with real users
4.  **PLANNED (Week 8):** Add confidence scores (only show high-confidence patterns)

**Contingency Plan:**
- If accuracy remains <70%: Rely more on GPT-4 for classification (increase cost but improve accuracy)
- Add user feedback mechanism: "Was this accurate?" (use to improve rules)

**Monitoring:**
- Weekly golden set regression tests
- User feedback on accuracy

**Owner:** [ML Lead]  
**Next Review:** Week 5 (after golden set)

---

### High Priority Risks (Summary)

**üü° R3: GPT-4 Costs Exceed Budget**
- Current: $0.015/query
- Budget: $50 total for semester
- Mitigation: Prompt optimization (Week 5), caching (Week 6), switch to GPT-3.5 if needed
- Monitoring: Daily cost dashboard

**üü° R4: AST Parsing Fails on Edge Cases**
- Mitigation: Extensive testing (Week 5-6), graceful error handling
- Contingency: Skip analysis if parsing fails, still show execution results

**üü° R5: Users Don't Find Recommendations Helpful**
- Mitigation: User testing Week 7 & 14
- Contingency: Simplify to "practice similar problems" instead of personalized queue

---

## 6. Research Plan (Updated)

### Questions We Need to Answer

#### Q1: What code features best predict error types?

**Status:**  In Progress (Week 4)  
**Deadline:** Week 5  
**Approach:**
1. Analyze 30 submissions from golden set
2. Test which AST features correlate with errors (nested loop depth, conditional branches, function calls)
3. Refine rule-based classifier

**Success Criteria:** Achieve 75%+ accuracy on golden set

---

#### Q2: Do users trust AI-generated feedback?

**Status:** Planned (Week 7)  
**Deadline:** Week 7 (user testing)  
**Approach:**
1. User testing: Show feedback for 3 problems
2. Ask: "Do you agree with this assessment?"
3. Measure: Agreement rate

**Success Criteria:** >70% agreement rate

---

#### Q3: What makes a problem recommendation "relevant"?

**Status:** Planned (Week 8)  
**Deadline:** Week 8  
**Approach:**
1. Test multiple recommendation strategies (similar problems, gradually harder, target weakest area)
2. A/B test with users
3. Measure: Which strategy gets higher "helpful" ratings

**Success Criteria:** Identify strategy with >70% helpfulness rating

---

## 7. User Study Plan

### Research Ethics

**IRB Status:** No IRB needed - IRB Light Checklist completed

**Consent & Privacy:**
- Verbal consent via Zoom
- Anonymized data (Participant 1, 2, etc.)
- Code submissions deleted after analysis
- No sensitive personal information collected

### User Testing Round 1: Week 7

**Participants:**
- **Sample size:** 5 CS students (juniors/seniors)
- **Recruitment:** CS department Slack, intro CS courses
- **Incentive:** $15 Starbucks gift card
- **Format:** Remote (Zoom), 45 minutes

**Testing Goals:**
1. Validate core submit ‚Üí feedback ‚Üí recommendation flow works
2. Measure perceived accuracy of weakness detection
3. Identify UX confusion points
4. Assess whether users would use this regularly

**Testing Tasks:**
1. Solve "Two Sum" problem (intentionally use O(n¬≤) solution) - 10 min
2. Review feedback - do you agree? - 5 min
3. Solve recommended next problem - 10 min
4. Review progress dashboard - 5 min
5. Post-task survey - 10 min

**Data Collection:**
- Task completion rate
- Time on task
- Think-aloud observations
- Survey: Accuracy agreement, helpfulness, satisfaction (5-point scale)
- Would you use this regularly? (Yes/No)

**Success Criteria:**
- >75% task completion rate
- >70% agree weaknesses are accurate
- >3.5/5.0 satisfaction score
- At least 3 actionable insights for improvement

**Timeline:**
- Days 1-2: Recruit 5 participants
- Days 3-5: Conduct sessions
- Days 6-7: Analyze findings

**Deliverable:** User Testing Report (due end of Week 7)

### User Testing Round 2: Week 14

**Participants:** 5 NEW students  
**Goal:** Validate improvements from Round 1 worked  
**Success Criteria:** >80% completion, >4.0/5.0 satisfaction

---

## 8. Project Timeline (Realistic)

### Weekly Breakdown

| Week | Focus | Deliverables | Status |
|------|-------|-------------|--------|
| 1-2 | Planning | Proposal v1, team contract | Complete |
| 3 | Prototype | Basic code execution working | Complete |
| 4 | **Design Review** | **Proposal v2, architecture diagram, eval plan** | **In Progress** |
| 5 | Golden Set | 30 problems with solutions for testing | Planned |
| 6 | Core Features | AST analysis, pattern detection, basic recommendations | Planned |
| 7 | **User Testing 1** | Test with 5 users, collect feedback | Planned |
| 8 | Iteration | Fix issues from testing, improve accuracy | Planned |
| 9 | **Midterms** | Reduced project work this week | Planned |
| 10 | Dashboard | Progress tracking, visualization | Planned |
| 11 | **Safety Audit** | Test edge cases, security, cost controls | Planned |
| 12 | Recommendations | Personalized problem queue algorithm | Planned |
| 13 | Polish | Bug fixes, performance optimization | Planned |
| 14 | **User Testing 2** | Final validation (5 new users) | Planned |
| 15 | **Final Demo** | Presentation, video, case study | Planned |

### Critical Path

**Must Complete in Order:**
1. Week 5: Golden set ‚Üí Blocks Week 6 testing
2. Week 6: Core features ‚Üí Blocks Week 7 user testing
3. Week 7: User testing ‚Üí Informs Week 8 improvements
4. Week 10: Dashboard ‚Üí Needed for Week 14 testing

### Backup Plan (Scope Cuts)

**If we fall behind, cut in this order:**

**Priority 3: Cut First**
1. Progress dashboard visualization ‚Üí Just show numbers
2. Multiple language support ‚Üí Python only is fine
3. Collaborative filtering recommendations ‚Üí Simple rule-based is okay

**Priority 2: Cut if Desperate**
4. Adaptive difficulty ‚Üí All problems same difficulty
5. Detailed explanations ‚Üí Just show error type

**Priority 1: Never Cut**
- Core submit ‚Üí execute ‚Üí feedback flow
- Pattern detection (even if simple)
- Basic recommendations
- User authentication

### Velocity Reality Check

**Week 3 Lessons:**
- Planned: 30 hours, complete prototype
- Actual: 25 hours, basic prototype (Judge0 integration took longer than expected)
- Lesson: Code execution is complex - need more buffer time

**Realistic Capacity:**
- Team member 1: 12 hours/week
- Team member 2: 10 hours/week
- Team member 3: 15 hours/week
- **Total: ~37 hours/week**

---

## 9. Team Health (Week 4 Check-In)

### What's Working

**Communication:** Daily Slack check-ins, quick responses  
**Technical Progress:** Prototype working, Judge0 integrated  
**Collaboration:** Pair programming 2x/week effective

### What Needs Improvement

**Testing:** No automated tests yet (allocate Week 6)  
**Documentation:** Code comments incomplete (spend 2 hours Week 5)  
**Workload:** Team member 2 has heavier course load (need cross-training)

### Updated Team Contract

**Meeting Schedule:**
- 2x/week in-person (Wed 6-8pm, Sat 10am-12pm)
- Daily async Slack check-ins

**Roles (Adjusted):**
- [Name 1]: Backend + Judge0 integration
- [Name 2]: Frontend + UX (reduced hours Week 9)
- [Name 3]: AST analysis + testing (new focus)

**Contingency for Week 9 (Midterms):**
- Frontload critical work to Week 5-8
- Cross-train on frontend (Week 5)
- Only non-blocking tasks in Week 9

---

## 10. Summary: Key Changes Since Week 2

### Problem & Solution
- **Validated problem** with 4 CS students
- **Narrowed target** to CS students preparing for FAANG interviews

### Technical Architecture
- **Simplified ML approach:** Custom model ‚Üí Rule-based + GPT-4
- **Reduced scope:** 13 error types ‚Üí 6 core types
- **Confirmed execution:** Judge0 API working (with occasional issues)
- **Removed JavaScript** from MVP (Python only)

### Success Metrics
- **Added specific targets** (75% accuracy, <10s latency, 4.0/5.0 satisfaction)
- **Baseline measurements** from Week 4 testing

### Risk Management
- **Identified Judge0 reliability** as #1 risk
- **Implemented rate limiting** and cost tracking
- **Created mitigation plans** for all critical risks

### Realistic Timeline
- **Adjusted for Week 3 velocity** (~25 hours actual vs 30 planned)
- **Buffer time for midterms** (Week 9)
- **Backup scope cuts** defined

---

## Review Checklist

- [x] All sections updated from Week 2
- [x] Specific changes documented with rationale
- [x] Measurable success metrics with targets
- [x] Architecture diagram included
- [x] All known risks documented
- [x] Realistic timeline based on Week 3-4 velocity
- [x] Team health assessment completed
- [ ] All team members reviewed and approved
- [ ] Proofread for typos
- [ ] Ready to submit

---

**Document Version:** 2.0  
**Last Updated:** October 25, 2025 (Week 4)  
**Next Review:** Week 7 (after user testing)
